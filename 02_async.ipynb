{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp asink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d773712-12fe-440e-891f-36f59666dfde",
   "metadata": {},
   "source": [
    "# The async version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f6471-8061-4fdd-85a1-25fdc27c5cf3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033c76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, typing, mimetypes, base64, json\n",
    "from collections import abc\n",
    "try: from IPython import display\n",
    "except: display=None\n",
    "\n",
    "from anthropic import AsyncAnthropic, AsyncAnthropicBedrock, AsyncAnthropicVertex\n",
    "from anthropic.types import Usage, TextBlock, Message, ToolUseBlock\n",
    "from anthropic.resources import messages\n",
    "\n",
    "import toolslm\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "from claudette.core import *\n",
    "from msglm import mk_msg_anthropic as mk_msg, mk_msgs_anthropic as mk_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13866a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4d81",
   "metadata": {},
   "source": [
    "## Async SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b53a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[1]\n",
    "cli = AsyncAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I'm Jeremy\"\n",
    "m = mk_msg(prompt)\n",
    "r = await cli.messages.create(messages=[m], model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cli.messages.create(messages=msgs, model=model, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b873aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AsyncClient(Client):\n",
    "    def __init__(self, model, cli=None, log=False, cache=False):\n",
    "        \"Async Anthropic messages client.\"\n",
    "        self.model,self.use = model,usage()\n",
    "        self.text_only = model in text_only_models\n",
    "        self.log = [] if log else None\n",
    "        self.c = (cli or AsyncAnthropic(default_headers={'anthropic-beta': 'prompt-caching-2024-07-31'}))\n",
    "        self.cache = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c7cb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "async def _log(self:AsyncClient, final, prefill, msgs, maxtok=None, sp=None, temp=None, stream=None, stop=None, **kwargs):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self._r(final, prefill)\n",
    "    if self.log is not None: self.log.append({\n",
    "        \"msgs\": msgs, \"prefill\": prefill,\n",
    "        \"maxtok\": maxtok, \"sp\": sp, \"temp\": temp, \"stream\": stream, \"stop\": stop, **kwargs,\n",
    "        \"result\": self.result, \"use\": self.use, \"stop_reason\": self.stop_reason, \"stop_sequence\": self.stop_sequence\n",
    "    })\n",
    "    return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01e9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = AsyncClient(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6520a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "async def _stream(self:AsyncClient, msgs:list, prefill='', **kwargs):\n",
    "    \"Stream response from Claude.\"\n",
    "    async with self.c.messages.stream(model=self.model, messages=mk_msgs(msgs, cache=self.cache, cache_last_ckpt_only=self.cache), **kwargs) as s:\n",
    "        if prefill: yield prefill\n",
    "        async for o in s.text_stream: yield o\n",
    "        await self._log(await s.get_final_message(), prefill, msgs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55566245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _precall(self:AsyncClient, msgs, prefill, stop, kwargs):\n",
    "    \"Prepare messages for a call to Claude.\"\n",
    "    pref = [prefill.strip()] if prefill else []\n",
    "    if not isinstance(msgs,list): msgs = [msgs]\n",
    "    if stop is not None:\n",
    "        if not isinstance(stop, (list)): stop = [stop]\n",
    "        kwargs[\"stop_sequences\"] = stop\n",
    "    msgs = mk_msgs(msgs+pref, cache=self.cache, cache_last_ckpt_only=self.cache)\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "835638bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "async def __call__(self:AsyncClient,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp='', # The system prompt\n",
    "             temp=0, # Temperature\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             prefill='', # Optional prefill to pass to Claude as start of its response\n",
    "             stream:bool=False, # Stream response?\n",
    "             stop=None, # Stop sequence\n",
    "             tools:Optional[list]=None, # List of tools to make available to Claude\n",
    "             tool_choice:Optional[dict]=None, # Optionally force use of some tool\n",
    "             **kwargs):\n",
    "    \"Make an async call to Claude.\"\n",
    "    if tools: kwargs['tools'] = [get_schema(o) for o in listify(tools)]\n",
    "    if tool_choice: kwargs['tool_choice'] = mk_tool_choice(tool_choice)\n",
    "    msgs = self._precall(msgs, prefill, stop, kwargs)\n",
    "    if any(t == 'image' for t in get_types(msgs)): assert not self.text_only, f\"Images are not supported by the current model type: {self.model}\"\n",
    "    if stream: return self._stream(msgs, prefill=prefill, max_tokens=maxtok, system=sp, temperature=temp, **kwargs)\n",
    "    res = await self.c.messages.create(\n",
    "        model=self.model, messages=msgs, max_tokens=maxtok, system=sp, temperature=temp, **kwargs)\n",
    "    return await self._log(res, prefill, msgs, maxtok, sp, temp, stream=stream, stop=stop, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = AsyncClient(model, log=True)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1220856",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.model = models[1]\n",
    "await c('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f479429",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Concisely, what is the meaning of life?\"\n",
    "pref = 'According to Douglas Adams,'\n",
    "await c(q, prefill=pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0230a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for o in await c(q, prefill=pref, stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int=1 # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d51f2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\"\n",
    "sp = \"You are a summing expert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff81d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[sums]\n",
    "choice = mk_tool_choice('sums')\n",
    "choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51966555",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(pr)\n",
    "r = await c(msgs, sp=sp, tools=tools, tool_choice=choice)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = mk_toolres(r, ns=globals())\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0862b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs += tr\n",
    "r = contents(await c(msgs, sp=sp, tools=sums))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7af80",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68060dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(Client.structured)\n",
    "async def structured(self:AsyncClient,\n",
    "               msgs:list, # List of messages in the dialog\n",
    "               tools:Optional[list]=None, # List of tools to make available to Claude\n",
    "               obj:Optional=None, # Class to search for tools  \n",
    "               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    tools = listify(tools)\n",
    "    res = await self(msgs, tools=tools, tool_choice=tools, **kwargs)\n",
    "    if ns is None: ns=mk_ns(*tools)\n",
    "    if obj is not None: ns = mk_ns(obj)\n",
    "    cts = getattr(res, 'content', [])\n",
    "    tcs = [call_func(o.name, o.input, ns=ns) for o in cts if isinstance(o,ToolUseBlock)]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a71b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await c.structured(pr, sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea144b8",
   "metadata": {},
   "source": [
    "## AsyncChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a77d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AsyncChat(Chat):\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp='', # Optional system prompt\n",
    "                 tools:Optional[list]=None, # List of tools to make available to Claude\n",
    "                 temp=0, # Temperature\n",
    "                 cont_pr:Optional[str]=None, # User prompt to continue an assistant response: assistant,[user:\"...\"],assistant\n",
    "                 cache: bool = False):\n",
    "        \"Anthropic async chat client.\"\n",
    "        assert model or cli\n",
    "        assert cont_pr != \"\", \"cont_pr may not be an empty string\"\n",
    "        self.c = (cli or AsyncClient(model, cache=cache))\n",
    "        if tools: tools = [tool(t) for t in tools]\n",
    "        self.h,self.sp,self.tools,self.cont_pr,self.temp,self.cache = [],sp,tools,cont_pr,temp,cache\n",
    "\n",
    "    @property\n",
    "    def use(self): return self.c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf77df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def cost(self: AsyncChat) -> float: return self.c.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b837c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = \"Never mention what tools you use.\"\n",
    "chat = AsyncChat(model, sp=sp)\n",
    "chat.c.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a05df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "async def _stream(self:AsyncChat, res):\n",
    "    \"Handle streaming response from Claude.\"\n",
    "    async for o in res: yield o\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools, obj=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e5ab3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "async def _append_pr(self:AsyncChat,\n",
    "               pr=None,  # Prompt / message\n",
    "              ):\n",
    "    \"Append prompt to history, handling role alternation.\"\n",
    "    prev_role = nested_idx(self.h, -1, 'role') if self.h else 'assistant' # First message should be 'user'\n",
    "    if pr and prev_role == 'user': await self() # already user request pending\n",
    "    self._post_pr(pr, prev_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f275336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports \n",
    "@patch\n",
    "def _post_pr(self:AsyncChat, pr, prev_role):\n",
    "    \"Process prompt after role check.\"\n",
    "    if pr is None and prev_role == 'assistant':\n",
    "        if self.cont_pr is None:\n",
    "            raise ValueError(\"Prompt must be given after assistant completion, or use `self.cont_pr`.\")\n",
    "        pr = self.cont_pr # No user prompt, keep the chain\n",
    "    if pr: self.h.append(mk_msg(pr, cache=self.cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bec85e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "async def __call__(self:AsyncChat,\n",
    "             pr=None,  # Prompt / message\n",
    "             temp=None, # Temperature\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream=False, # Stream response?\n",
    "             prefill='', # Optional prefill to pass to Claude as start of its response\n",
    "             tool_choice:Optional[dict]=None, # Optionally force use of some tool\n",
    "             **kw):\n",
    "    \"Make an async call to Claude via the chat interface.\"\n",
    "    if temp is None: temp=self.temp\n",
    "    await self._append_pr(pr)\n",
    "    res = await self.c(self.h, stream=stream, prefill=prefill, sp=self.sp, temp=temp, maxtok=maxtok,\n",
    "                 tools=self.tools, tool_choice=tool_choice,**kw)\n",
    "    if stream: return self._stream(res)\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"I'm Jeremy\")\n",
    "await chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a32de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Concisely, what is the meaning of life?\"\n",
    "pref = 'According to Douglas Adams,'\n",
    "await chat(q, prefill=pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529104ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model, sp=sp)\n",
    "async for o in await chat(\"I'm Jeremy\", stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6535cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = f\"What is {a}+{b}?\"\n",
    "chat = AsyncChat(model, sp=sp, tools=[sums])\n",
    "r = await chat(pr)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc85163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr += \" Say the answer in a sentence.\"\n",
    "chat = AsyncChat(model, sp=sp, tools=[sums])\n",
    "r = await chat(pr)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7db90",
   "metadata": {},
   "source": [
    "# Async Tool Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a837675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(AsyncChat.__call__)\n",
    "async def toolloop(self:AsyncChat,\n",
    "             pr, # Prompt to pass to Claude\n",
    "             max_steps=10, # Maximum number of tool requests to loop through  \n",
    "             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n",
    "             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response from Claude, automatically following up with `tool_use` messages\"\n",
    "    n_msgs = len(self.h)\n",
    "    r = await self(pr, **kwargs)\n",
    "    for i in range(max_steps):\n",
    "        if r.stop_reason!='tool_use': break\n",
    "        if trace_func: trace_func(self.h[n_msgs:]); n_msgs = len(self.h)\n",
    "        r = await self(**kwargs)\n",
    "        if not (cont_func or noop)(self.h[-2]): break\n",
    "    if trace_func: trace_func(self.h[n_msgs:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24decda9",
   "metadata": {},
   "source": [
    "Let's create some data to test out async tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b7fb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n",
    "    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n",
    "    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n",
    "\n",
    "customers = {\n",
    "    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n",
    "               orders=[orders['O1'], orders['O2']]),\n",
    "    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n",
    "               orders=[orders['O3']])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ee6dd",
   "metadata": {},
   "source": [
    "Next we define the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d035b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_info(\n",
    "    customer_id:str # ID of the customer\n",
    "): # Customer's name, email, phone number, and list of orders\n",
    "    \"Retrieves a customer's information and their orders based on the customer ID\"\n",
    "    print(f'- Retrieving customer {customer_id}')\n",
    "    return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "def get_order_details(\n",
    "    order_id:str # ID of the order\n",
    "): # Order's ID, product name, quantity, price, and order status\n",
    "    \"Retrieves the details of a specific order based on the order ID\"\n",
    "    print(f'- Retrieving order {order_id}')\n",
    "    return orders.get(order_id, \"Order not found\")\n",
    "\n",
    "def cancel_order(\n",
    "    order_id:str # ID of the order to cancel\n",
    ")->bool: # True if the cancellation is successful\n",
    "    \"Cancels an order based on the provided order ID\"\n",
    "    print(f'- Cancelling order {order_id}')\n",
    "    if order_id not in orders: return False\n",
    "    orders[order_id]['status'] = 'Cancelled'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74395175",
   "metadata": {},
   "source": [
    "We can see the async tool loop in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ff19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tools and chat\n",
    "tools = [get_customer_info, get_order_details, cancel_order]\n",
    "chat = AsyncChat(model, tools=tools)\n",
    "\n",
    "# Example using async toolloop with a simple query\n",
    "async def example_email_query():\n",
    "    r = await chat.toolloop('Can you tell me the email address for customer C1?')\n",
    "    print(contents(r))\n",
    "\n",
    "# Example with a more complex multi-step tool process\n",
    "async def example_cancel_orders():\n",
    "    chat = AsyncChat(model, tools=tools)\n",
    "    r = await chat.toolloop('Please cancel all orders for customer C1 for me.', trace_func=print)\n",
    "    print(contents(r))\n",
    "    \n",
    "    # Verify the order was canceled\n",
    "    r = await chat.toolloop('What is the status of order O2?')\n",
    "    print(contents(r))\n",
    "\n",
    "# Execute the examples\n",
    "await example_email_query()\n",
    "await example_cancel_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec90f63",
   "metadata": {},
   "source": [
    "# Image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = Path('samples/puppy.jpg')\n",
    "img = fn.read_bytes()\n",
    "display.Image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0eed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "msg = mk_msg([img, q])\n",
    "await c([msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fea77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model, sp=sp, cache=True)\n",
    "await chat(\"Lorem ipsum dolor sit amet\" * 150)\n",
    "chat.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"Whoops, sorry about that!\")\n",
    "chat.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4289",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e9ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infrared-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
